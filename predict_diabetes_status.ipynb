{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Import necessary packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns # Heatmap viz \n",
    "from sklearn.preprocessing import StandardScaler # Standardize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "dataset = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "We will start by reviewing metadata, sample data, and descriptive statistics for our dataset. <br>\n",
    "This section also includes missing data identification/replacement with 'NaN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review dataframe metadata\n",
    "dataset.info()\n",
    "\n",
    "# Note: All features are numeric. No null values. Sample size relatively small (767 patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review first few rows of dataset\n",
    "dataset.head()\n",
    "\n",
    "# Note: Zeroes are present in fields that should be non-zero (e.g. Insulin = 0, SkinThickness = 0). We will correct in next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulting the original paper, it seems zeroes are used to indicate missing values. Let's replace zeroes for Glucose, BloodPressure, SkinThickness, Insulin, and BMI variables with NaNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\n",
    "dataset[columns] = dataset[columns].replace(0,np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for 9 variables.\n",
    "fig, axis = plt.subplots(3,3,figsize=(15, 15))\n",
    "dataset.hist(ax=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that the dataset is imbalanced - There are almost twice as many non-diabetics vs. diabetics (Outcome == 0 vs. == 1). <br>\n",
    "After dividing up our train and test datasets, we will subsample the train dataset to resolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe(include='all')\n",
    "\n",
    "#Note: All values are reasonable - I will opt against outlier removal and StandardScalar should work fine for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review correlations\n",
    "corr = dataset.corr()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top correlation pairs: \n",
    "* BMI and SkinThickness (0.64)\n",
    "* Insulin and Glucose (0.58)\n",
    "* Age and Pregnancies (0.54)\n",
    "* Glucose and Outcome (0.49)\n",
    "\n",
    "Because correlation coefficients are well below < 0.95, I feel comfortable moving forward without variable removal/manipulation. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Datasets\n",
    "\n",
    "Next, we will standardize our features (all numeric) using StandardScaler - this removes the mean and scales to unit variance. <br>\n",
    "We will also divide the data into train and test datasets, and correct for Outcome class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "We will test a few approaches, maybe these:\n",
    "* Logistic Regression\n",
    "* Neural Networks\n",
    "* XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
